{
	"name": "1_bronze_transform_listings",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4ac76acf-cc8b-48f5-b042-90ebd0471d8f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/dc9c4ff8-158c-4aad-afb2-2ded35f739f6/resourceGroups/rg-primaryresourcegroup/providers/Microsoft.Synapse/workspaces/ws-primary/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://ws-primary.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# This notebook performs the following tasks:\r\n",
					"- #### Reads all files from the 'raw' container in storage, modified on the current day\r\n",
					"- #### Create year, month, and day columns for partitioning based on the input_file_name()\r\n",
					"- #### Drop duplicates listings from  the data (this does happen sometimes)\r\n",
					"- #### Append current date's data to the realtor_lake.listings table\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Declare variables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.types import *\r\n",
					"from datetime import datetime, date\r\n",
					"from delta.tables import *\r\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"bronze_container = \"abfss://realtor@dlsprimarystorage.dfs.core.windows.net/bronze\"\r\n",
					"silver_container = \"abfss://realtor@dlsprimarystorage.dfs.core.windows.net/silver\""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# spark.conf.set(\"spark.storage.synapse.linkedServiceName\", \"ws-primary-WorkspaceDefaultStorage\")\r\n",
					"# spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
					"# df = spark.read.csv('abfss://realtor@dlsprimarystorage.dfs.core.windows.net/bronze')\r\n",
					"# df.show()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# %%sql\r\n",
					"# SELECT COUNT(1)\r\n",
					"# FROM realtor_lake.listings .strftime('%Y-%m-%dT%H:%M:%S')"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"today = date.today().strftime('%Y-%m-%dT%H:%M:%S')\r\n",
					"now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\r\n",
					"print(today)\r\n",
					"print(type(today))\r\n",
					"print(now)\r\n",
					"print(type(now))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# listings_df = spark.read.option(\"modifiedAfter\", today).parquet(bronze_container + \"/*/*/*/*\")\r\n",
					"listings_df = spark.read.parquet(bronze_container + \"/*/*/*/*\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"@F.udf\r\n",
					"def get_datetime(in_string):\r\n",
					"    return in_string.split(\"/\")[-1].split(\"_\")[0]\r\n",
					"\r\n",
					"last_modified_time = listings_df.withColumn(\"last_modified_time\", F.lit(now))\r\n",
					"\r\n",
					"filepath_df = last_modified_time.withColumn(\"source_file\", F.input_file_name())\r\n",
					"\r\n",
					"filepath_df = filepath_df.withColumn(\"source_file\", F.substring(get_datetime(F.col(\"source_file\")),1,10))\r\n",
					"\r\n",
					"year_month_day_df = filepath_df.withColumn(\"year\", F.substring(F.col(\"source_file\"), 1, 4)) \\\r\n",
					"                                .withColumn(\"month\", F.substring(F.col(\"source_file\"), 6, 2)) \\\r\n",
					"                                .withColumn(\"day\", F.substring(F.col(\"source_file\"), 9, 2))\r\n",
					"\r\n",
					"\r\n",
					"#display(year_month_day_df.limit(5))\r\n",
					"print(year_month_day_df.count())"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## OPTION 1:  Use Native dataframe method to persist data\r\n",
					"#### Partition by Year, Month, and Day and persist into ADLS"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# year_month_day_df.write.mode(\"overwrite\").parquet(silver_container + \"/listings\")\r\n",
					"# year_month_day_df.write.mode(\"overwrite\").partitionBy(\"Year\", \"Month\", \"Day\").parquet(silver_container + \"/listings\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## OPTION 2:  Use Spark SQL to persist data\r\n",
					"#### Create external table via Spark SQL and persist data into it and then transform ?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"drop_duplicates_df = year_month_day_df.dropDuplicates([\"ListingId\", \"MlsNumber\"])\r\n",
					"distinct_df = year_month_day_df.distinct()\r\n",
					"# display(distinct_df.limit(25))\r\n",
					"# print(\"Total count: \" + str(year_month_day_df.count()))\r\n",
					"# print(\"Dropped duplicate count: \" + str(drop_duplicates_df.count()))\r\n",
					"# print(\"Distinct count: \" + str(distinct_df.count()))"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"check_df = distinct_df.select(F.col(\"source_file\"), F.col('BuildingType')).distinct()\r\n",
					"check_df.show()"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Parquet does not support update or delete operations\r\n",
					"# The last step with DELETE FROM will throw: \"Delete is only supported with v2 tables\"\r\n",
					"# Solution:  Convert parquet to delta tables to perform deletes\r\n",
					"\r\n",
					"# for f in check_df.collect():\r\n",
					"#     source_file = f['source_file']\r\n",
					"#     building_type = f['BuildingType']\r\n",
					"#     spark.sql(\"DELETE FROM delta.`synapse/workspaces/ws-primary/warehouse/realtor_lake.db/listings/` WHERE source_file = '{0}' AND BuildingType = '{1}'\".format(source_file, building_type))"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# deltaTable = DeltaTable.forPath(spark, '/synapse/workspaces/ws-primary/warehouse/realtor_lake.db/listings')"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Temp workaround without delta tables: Delete the files related to the dates from the Synapse workspace folder in ADLS\r\n",
					"# # Example:  folder_path = \"/synapse/workspaces/ws-primary/warehouse/realtor_lake.db/listings/year={0}/month={1}/day={2}/\".format(v_year, v_month, v_day)\r\n",
					"\r\n",
					"# # Delete the parquet files from storage\r\n",
					"\r\n",
					"# for f in check_df.collect():\r\n",
					"#     v_source_file = f['source_file']\r\n",
					"#     v_building_type = f['BuildingType']\r\n",
					"\r\n",
					"\r\n",
					"# # You can't call Spark functions on Python strings. You need to use Python string methods, e.g., dataCollect[:3] for first 3 chars\r\n",
					"# v_year = v_source_file[:4]\r\n",
					"# v_month = v_source_file[5:7:1]\r\n",
					"# v_day = v_source_file[8:11:1]\r\n",
					"# print(v_month)\r\n",
					"\r\n",
					"# # Delete today's files if exists (.rm method)\r\n",
					"# folder_path = \"/synapse/workspaces/ws-primary/warehouse/realtor_lake.db/listings_bronze/year={0}/month={1}/day={2}/\".format(v_year, v_month, v_day)\r\n",
					"# if mssparkutils.fs.exists(folder_path):\r\n",
					"#     print(v_year)\r\n",
					"#     mssparkutils.fs.rm(folder_path, True)\r\n",
					"\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS realtor_lake\")\r\n",
					"# year_month_day_df.write.mode(\"overwrite\").saveAsTable(\"realtor_spark.listings\")\r\n",
					"distinct_df.write.mode(\"append\").partitionBy(\"year\", \"month\", \"day\").saveAsTable(\"realtor_lake.listings_bronze_new\")"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import datetime\r\n",
					"# import time\r\n",
					"# ticks = 638229561089970000\r\n",
					"# print(datetime.datetime(1, 1, 1) + datetime.timedelta(microseconds = ticks //10))"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# -- %%sql\r\n",
					"# WITH cte AS\r\n",
					"# (\r\n",
					"#  SELECT ListingId, MlsNumber, year, month, day, row_number() OVER (ORDER BY ListingId, MlsNumber DESC) rnum\r\n",
					"#  FROM realtor_lake.listings) \r\n",
					"#  DELETE FROM cte WHERE rnum > 1\r\n",
					"#  DROP TABLE realtor_lake.listings\r\n",
					"#  DROP DATABASE realtor_spark"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"SELECT *\r\n",
					"FROM realtor_lake.listings_bronze_new"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}